{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tweets chargés: 7136\n",
      "Exemples des 3 premiers tweets:\n",
      "Tweet: Saint-Nazaire se rêve en capitale des #énergies marines renouvelables #electricite http://t.co/nArcLYqxLd\n",
      "Étiquette: =\n",
      "\n",
      "Tweet: 4eme Conférence internationale sur le changement climatique et le développement http://t.co/biVoGZVlLj\n",
      "Étiquette: =\n",
      "\n",
      "Tweet: Rencontres #windustry 2014 Sascha Wiesner décrit les innovations d'A2SEA dans le transport des éoliennes offshore http://t.co/hPD0DQPay6\n",
      "Étiquette: =\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour charger les données\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if '\\t' in line:\n",
    "                text, label = line.strip().split('\\t')\n",
    "                data.append((text, label))\n",
    "    return data\n",
    "\n",
    "# Testons la fonction\n",
    "train_data = load_data('../data/train.txt')\n",
    "print(f\"Nombre de tweets chargés: {len(train_data)}\")\n",
    "print(\"Exemples des 3 premiers tweets:\")\n",
    "for i in range(min(3, len(train_data))):\n",
    "    print(f\"Tweet: {train_data[i][0]}\")\n",
    "    print(f\"Étiquette: {train_data[i][1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  Saint-Nazaire se rêve en capitale des #énergie...     =\n",
      "1  4eme Conférence internationale sur le changeme...     =\n",
      "2  Rencontres #windustry 2014 Sascha Wiesner décr...     =\n",
      "3  #Photos :Dans l’Ouest américain,les stigmates ...     -\n",
      "4  Parc #éolien: entente conclut entre Port-Carti...     +\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_data, columns=['text', 'label'])\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des étiquettes:\n",
      "label\n",
      "=    3205\n",
      "+    2234\n",
      "-    1697\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pourcentage par étiquette:\n",
      "label\n",
      "=    44.91\n",
      "+    31.31\n",
      "-    23.78\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Exemples de tweets par catégorie:\n",
      "\n",
      "Catégorie: +\n",
      "- Parc #éolien: entente conclut entre Port-Cartier la compagnie Système d'énergie renouvelable Canada: http://t.co/G9DWR1T792 #CoteNord\n",
      "- Frédéric Chauvel : \"attirer les investisseurs en créant un écosystème favorisant\" #AFE2014\n",
      "\n",
      "Catégorie: -\n",
      "- #Photos :Dans l’Ouest américain,les stigmates d’une sécheresse historique http://t.co/F6As9walnX #Environnement #Climat #EtatsUnis #Ecologie\n",
      "- Oublié le tps où Le Drian écrivait au ministre de la défense se plaigant de l’armée empêchant dvpt de l’éolien en Bzh http://t.co/JTlx2ngbe4\n",
      "\n",
      "Catégorie: =\n",
      "- Saint-Nazaire se rêve en capitale des #énergies marines renouvelables #electricite http://t.co/nArcLYqxLd\n",
      "- 4eme Conférence internationale sur le changement climatique et le développement http://t.co/biVoGZVlLj\n"
     ]
    }
   ],
   "source": [
    "# Distribution des étiquettes\n",
    "label_counts = train_df['label'].value_counts()\n",
    "print(\"Distribution des étiquettes:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Pourcentage\n",
    "label_percentages = 100 * label_counts / len(train_df)\n",
    "print(\"\\nPourcentage par étiquette:\")\n",
    "print(label_percentages.round(2))\n",
    "\n",
    "# Exemples de tweets pour chaque catégorie\n",
    "print(\"\\nExemples de tweets par catégorie:\")\n",
    "for label in ['+', '-', '=']:\n",
    "    print(f\"\\nCatégorie: {label}\")\n",
    "    examples = train_df[train_df['label'] == label]['text'].head(2).values\n",
    "    for ex in examples:\n",
    "        print(f\"- {ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemples de prétraitement:\n",
      "Original: Saint-Nazaire se rêve en capitale des #énergies marines renouvelables #electricite http://t.co/nArcLYqxLd\n",
      "Prétraité: saint nazaire se rêve en capitale des énergies marines renouvelables electricite http t co narclyqxld\n",
      "\n",
      "Original: 4eme Conférence internationale sur le changement climatique et le développement http://t.co/biVoGZVlLj\n",
      "Prétraité: 4eme conférence internationale sur le changement climatique et le développement http t co bivogzvllj\n",
      "\n",
      "Original: Rencontres #windustry 2014 Sascha Wiesner décrit les innovations d'A2SEA dans le transport des éoliennes offshore http://t.co/hPD0DQPay6\n",
      "Prétraité: rencontres windustry 2014 sascha wiesner décrit les innovations d a2sea dans le transport des éoliennes offshore http t co hpd0dqpay6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fonction de prétraitement basique\n",
    "def preprocess_text(text):\n",
    "    # Conversion en minuscules\n",
    "    text = text.lower()\n",
    "    # Suppression de la ponctuation et caractères spéciaux\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Suppression des espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Testons le prétraitement sur quelques exemples\n",
    "print(\"Exemples de prétraitement:\")\n",
    "for i in range(3):\n",
    "    original = train_df['text'].iloc[i]\n",
    "    processed = preprocess_text(original)\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Prétraité: {processed}\")\n",
    "    print()\n",
    "\n",
    "# Ajoutons une colonne avec les textes prétraités\n",
    "train_df['processed_text'] = train_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 mots pour les tweets positifs:\n",
      "[('t', 1814), ('co', 1810), ('http', 1785), ('de', 1261), ('l', 1027), ('la', 840), ('le', 791), ('les', 731), ('des', 555), ('et', 510)]\n",
      "\n",
      "Top 10 mots pour les tweets négatifs:\n",
      "[('t', 1242), ('co', 1228), ('http', 1210), ('de', 1052), ('l', 773), ('la', 688), ('le', 649), ('les', 591), ('des', 507), ('et', 413)]\n",
      "\n",
      "Top 10 mots pour les tweets neutres:\n",
      "[('t', 2704), ('co', 2690), ('http', 2655), ('de', 2006), ('l', 1167), ('la', 1076), ('le', 1009), ('des', 803), ('et', 759), ('les', 756)]\n"
     ]
    }
   ],
   "source": [
    "# Initialisation des compteurs pour chaque catégorie\n",
    "positive_words = Counter()\n",
    "negative_words = Counter()\n",
    "neutral_words = Counter()\n",
    "\n",
    "# Remplissage des compteurs\n",
    "for _, row in train_df.iterrows():\n",
    "    words = row['processed_text'].split()\n",
    "    \n",
    "    if row['label'] == '+':\n",
    "        positive_words.update(words)\n",
    "    elif row['label'] == '-':\n",
    "        negative_words.update(words)\n",
    "    else:\n",
    "        neutral_words.update(words)\n",
    "\n",
    "# Affichage des mots les plus fréquents pour chaque catégorie\n",
    "print(\"Top 10 mots pour les tweets positifs:\")\n",
    "print(positive_words.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 mots pour les tweets négatifs:\")\n",
    "print(negative_words.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 mots pour les tweets neutres:\")\n",
    "print(neutral_words.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 mots positifs (filtrés):\n",
      "[('co', 1810), ('http', 1785), ('énergie', 289), ('climatique', 195), ('écosystème', 194), ('solaire', 189), ('via', 181), ('renouvelables', 176), ('écologie', 174), ('énergies', 170)]\n",
      "\n",
      "Top 10 mots négatifs (filtrés):\n",
      "[('co', 1228), ('http', 1210), ('climatique', 210), ('écologie', 197), ('via', 189), ('éoliennes', 143), ('ecologie', 142), ('réchauffement', 140), ('écologistes', 136), ('on', 129)]\n",
      "\n",
      "Top 10 mots neutres (filtrés):\n",
      "[('co', 2690), ('http', 2655), ('ecologie', 320), ('climatique', 311), ('via', 293), ('énergie', 271), ('durable', 263), ('développement', 259), ('écologie', 220), ('renouvelables', 218)]\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les mots très fréquents/communs qui n'apportent pas d'information\n",
    "# (articles, prépositions, etc.)\n",
    "common_words = {'le', 'la', 'les', 'des', 'et', 'en', 'du', 'de', 'un', 'une', \n",
    "                'à', 'au', 'aux', 'pour', 'dans', 'sur', 'par', 'avec', 'ce', \n",
    "                'cette', 'ces', 'est', 'sont', 'ont', 'qui', 'que', 'quoi', \n",
    "                'comment', 'pourquoi', 'où', 'quand', 'pas', 'plus', 'moins'}\n",
    "\n",
    "# Fonction pour filtrer les mots communs\n",
    "def filter_counter(counter, common_words):\n",
    "    return Counter({word: count for word, count in counter.items() \n",
    "                  if word not in common_words and len(word) > 1})\n",
    "\n",
    "# Application du filtre\n",
    "filtered_positive = filter_counter(positive_words, common_words)\n",
    "filtered_negative = filter_counter(negative_words, common_words)\n",
    "filtered_neutral = filter_counter(neutral_words, common_words)\n",
    "\n",
    "# Affichage des résultats filtrés\n",
    "print(\"Top 10 mots positifs (filtrés):\")\n",
    "print(filtered_positive.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 mots négatifs (filtrés):\")\n",
    "print(filtered_negative.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 mots neutres (filtrés):\")\n",
    "print(filtered_neutral.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests de classification:\n",
      "Texte: Parc #éolien: entente conclut entre Port-Cartier la compagnie Système d'énergie renouvelable Canada: http://t.co/G9DWR1T792 #CoteNord\n",
      "Prédiction: +\n",
      "\n",
      "Texte: #Photos :Dans l’Ouest américain,les stigmates d’une sécheresse historique http://t.co/F6As9walnX #Environnement #Climat #EtatsUnis #Ecologie\n",
      "Prédiction: =\n",
      "\n",
      "Texte: Saint-Nazaire se rêve en capitale des #énergies marines renouvelables #electricite http://t.co/nArcLYqxLd\n",
      "Prédiction: =\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fonction de classification par vocabulaire\n",
    "def classify_by_vocabulary(text, positive_vocab, negative_vocab, neutral_vocab):\n",
    "    # Prétraitement du texte\n",
    "    processed = preprocess_text(text)\n",
    "    words = processed.split()\n",
    "    \n",
    "    # Compter les mots dans chaque catégorie\n",
    "    pos_score = sum(1 for word in words if word in positive_vocab)\n",
    "    neg_score = sum(1 for word in words if word in negative_vocab)\n",
    "    neu_score = sum(1 for word in words if word in neutral_vocab)\n",
    "    \n",
    "    # Déterminer la catégorie avec le plus grand score\n",
    "    scores = {'+': pos_score, '-': neg_score, '=': neu_score}\n",
    "    max_score = max(scores.values())\n",
    "    \n",
    "    # Si aucun mot du vocabulaire n'est trouvé\n",
    "    if max_score == 0:\n",
    "        return '='  # Par défaut: neutre\n",
    "    \n",
    "    # En cas d'égalité, ordonner par priorité\n",
    "    if scores['='] == max_score:\n",
    "        return '='\n",
    "    elif scores['+'] == max_score:\n",
    "        return '+'\n",
    "    else:\n",
    "        return '-'\n",
    "\n",
    "# Convertir les compteurs en ensembles de vocabulaire (pour une recherche plus rapide)\n",
    "pos_vocab = set(word for word, _ in filtered_positive.most_common(100))\n",
    "neg_vocab = set(word for word, _ in filtered_negative.most_common(100))\n",
    "neu_vocab = set(word for word, _ in filtered_neutral.most_common(100))\n",
    "\n",
    "# Testons sur quelques exemples\n",
    "test_examples = [\n",
    "    train_df[train_df['label'] == '+']['text'].iloc[0],  # un exemple positif\n",
    "    train_df[train_df['label'] == '-']['text'].iloc[0],  # un exemple négatif\n",
    "    train_df[train_df['label'] == '=']['text'].iloc[0],  # un exemple neutre\n",
    "]\n",
    "\n",
    "print(\"Tests de classification:\")\n",
    "for example in test_examples:\n",
    "    predicted = classify_by_vocabulary(example, pos_vocab, neg_vocab, neu_vocab)\n",
    "    print(f\"Texte: {example}\")\n",
    "    print(f\"Prédiction: {predicted}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitude sur l'ensemble de développement: 0.5657\n",
      "Nombre de prédictions correctes: 396/700\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données de développement\n",
    "dev_data = load_data('../data/dev.txt')\n",
    "dev_df = pd.DataFrame(dev_data, columns=['text', 'label'])\n",
    "\n",
    "# Application de la classification\n",
    "predictions = []\n",
    "for text in dev_df['text']:\n",
    "    pred = classify_by_vocabulary(text, pos_vocab, neg_vocab, neu_vocab)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Calcul de l'exactitude (accuracy)\n",
    "correct = sum(1 for pred, true in zip(predictions, dev_df['label']) if pred == true)\n",
    "accuracy = correct / len(dev_df)\n",
    "\n",
    "print(f\"Exactitude sur l'ensemble de développement: {accuracy:.4f}\")\n",
    "print(f\"Nombre de prédictions correctes: {correct}/{len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitude avec pondération: 0.4900\n",
      "Nombre de prédictions correctes: 343/700\n"
     ]
    }
   ],
   "source": [
    "# Version améliorée avec des poids\n",
    "def classify_weighted(text, pos_counter, neg_counter, neu_counter):\n",
    "    # Prétraitement\n",
    "    processed = preprocess_text(text)\n",
    "    words = processed.split()\n",
    "    \n",
    "    # Calcul des scores pondérés par la fréquence\n",
    "    pos_score = sum(pos_counter.get(word, 0) for word in words)\n",
    "    neg_score = sum(neg_counter.get(word, 0) for word in words)\n",
    "    neu_score = sum(neu_counter.get(word, 0) for word in words)\n",
    "    \n",
    "    # Normalisation par le total de mots dans chaque catégorie\n",
    "    total_pos = sum(pos_counter.values())\n",
    "    total_neg = sum(neg_counter.values())\n",
    "    total_neu = sum(neu_counter.values())\n",
    "    \n",
    "    if total_pos > 0: pos_score /= total_pos\n",
    "    if total_neg > 0: neg_score /= total_neg\n",
    "    if total_neu > 0: neu_score /= total_neu\n",
    "    \n",
    "    # Détermination de la catégorie\n",
    "    scores = {'+': pos_score, '-': neg_score, '=': neu_score}\n",
    "    \n",
    "    if all(score == 0 for score in scores.values()):\n",
    "        return '='  # Par défaut\n",
    "    \n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Test de la version améliorée\n",
    "weighted_predictions = []\n",
    "for text in dev_df['text']:\n",
    "    pred = classify_weighted(text, filtered_positive, filtered_negative, filtered_neutral)\n",
    "    weighted_predictions.append(pred)\n",
    "\n",
    "weighted_correct = sum(1 for pred, true in zip(weighted_predictions, dev_df['label']) if pred == true)\n",
    "weighted_accuracy = weighted_correct / len(dev_df)\n",
    "\n",
    "print(f\"Exactitude avec pondération: {weighted_accuracy:.4f}\")\n",
    "print(f\"Nombre de prédictions correctes: {weighted_correct}/{len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de la méthode simple pour les prédictions finales\n",
      "Prédictions générées et enregistrées dans 'test-predict.txt'\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données de test\n",
    "test_data = load_data('../data/test.txt')\n",
    "test_df = pd.DataFrame(test_data, columns=['text', 'label'])  # label sera \"??\"\n",
    "\n",
    "# Génération des prédictions (utiliser la meilleure méthode entre les deux)\n",
    "if weighted_accuracy > accuracy:\n",
    "    print(\"Utilisation de la méthode pondérée pour les prédictions finales\")\n",
    "    test_predictions = []\n",
    "    for text in test_df['text']:\n",
    "        pred = classify_weighted(text, filtered_positive, filtered_negative, filtered_neutral)\n",
    "        test_predictions.append(pred)\n",
    "else:\n",
    "    print(\"Utilisation de la méthode simple pour les prédictions finales\")\n",
    "    test_predictions = []\n",
    "    for text in test_df['text']:\n",
    "        pred = classify_by_vocabulary(text, pos_vocab, neg_vocab, neu_vocab)\n",
    "        test_predictions.append(pred)\n",
    "\n",
    "# Création du fichier de sortie\n",
    "with open('../results/test-predict.txt', 'w', encoding='utf-8') as f:\n",
    "    for (text, _), prediction in zip(test_data, test_predictions):\n",
    "        f.write(f\"{text}\\t{prediction}\\n\")\n",
    "\n",
    "print(\"Prédictions générées et enregistrées dans 'test-predict.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure finale pour le script Python\n",
    "def main_function():\n",
    "    # Chargement et préparation des données d'entraînement\n",
    "    train_data = load_data('train.txt')\n",
    "    train_df = pd.DataFrame(train_data, columns=['text', 'label'])\n",
    "    train_df['processed_text'] = train_df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Construction des vocabulaires\n",
    "    positive_words = Counter()\n",
    "    negative_words = Counter()\n",
    "    neutral_words = Counter()\n",
    "    \n",
    "    for _, row in train_df.iterrows():\n",
    "        words = row['processed_text'].split()\n",
    "        if row['label'] == '+':\n",
    "            positive_words.update(words)\n",
    "        elif row['label'] == '-':\n",
    "            negative_words.update(words)\n",
    "        else:  # label == '='\n",
    "            neutral_words.update(words)\n",
    "    \n",
    "    # Filtrage des mots communs\n",
    "    common_words = {'le', 'la', 'les', 'des', 'et', 'en', 'du', 'de', 'un', 'une', \n",
    "                    'à', 'au', 'aux', 'pour', 'dans', 'sur', 'par', 'avec', 'ce', \n",
    "                    'cette', 'ces', 'est', 'sont', 'ont', 'qui', 'que', 'quoi', \n",
    "                    'comment', 'pourquoi', 'où', 'quand', 'pas', 'plus', 'moins'}\n",
    "    \n",
    "    filtered_positive = filter_counter(positive_words, common_words)\n",
    "    filtered_negative = filter_counter(negative_words, common_words)\n",
    "    filtered_neutral = filter_counter(neutral_words, common_words)\n",
    "    \n",
    "    # Création des vocabulaires pour la classification\n",
    "    pos_vocab = set(word for word, _ in filtered_positive.most_common(100))\n",
    "    neg_vocab = set(word for word, _ in filtered_negative.most_common(100))\n",
    "    neu_vocab = set(word for word, _ in filtered_neutral.most_common(100))\n",
    "    \n",
    "    # Lecture du fichier d'entrée et génération des prédictions\n",
    "    # (à adapter selon les arguments de ligne de commande)\n",
    "    input_file = 'dev.txt'  # Sera remplacé par sys.argv[1]\n",
    "    \n",
    "    input_data = load_data(input_file)\n",
    "    predictions = []\n",
    "    \n",
    "    for text, _ in input_data:\n",
    "        pred = classify_by_vocabulary(text, pos_vocab, neg_vocab, neu_vocab)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Écriture des résultats\n",
    "    for (text, _), prediction in zip(input_data, predictions):\n",
    "        print(f\"{text}\\t{prediction}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
